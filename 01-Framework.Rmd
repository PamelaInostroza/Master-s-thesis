---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Framework 

Two main topics are relevant to go deep into this chapter, the scales that want to be evaluated and the methodology to be used. For this, first, large scales assessments are going to be learn, along with their focus, data collection, and available information. The scales that are going to be used in this research are studied as well, these were selected mainly because the promotion of tolerance is an important goal of European education policies focused on democratic citizenship and human rights. It is expected that civilians agree to equal rights for all the residents of the same country. Based on late expressions of discrimination around the world against women and different ethnicities and race groups, particularly in Europe, this research will be focused on identifying which are the profiles of students according to their attitude towards these groups.  

To achieve this goal, the correct technique should be used, taking into account not only the expected result but also the complexity of the information and the most suitable estimation technique. In this chapter, mixture model's framework will be reviewed in order to identify the most suitable technique and statistics to consider when analyzing this data.  

Mixed models will be learned, particularly Latent Class Analysis which is a model-based approach to classify individuals/cases into distinctive groups, called latent classes, based on their responses to a set of observed categorical variables. This methodology requires multiple calculations that now, thanks to the increase of computing availability power and the creation of specialized software for mixture models, is more accessible to be used by researchers.   

## International Large-Scale Assessments 

International Large-Scale Assessments (ILSAs) have been used to draw comparisons among countries on a variety of topics in education and in adolescent development [@isac_indicators_2019]. These assessments can inform the public about influential factors on the micro and macro levels and provide important data for studying the context and processes of education and development. 

### IEA - ICCS 2016  

The International Association for the Evaluation of Educational Achievement (IEA) International Civic and Citizenship Education Study (ICCS) produces internationally comparative data collected via student, school, and teacher questionnaires. Data from different waves of the ICCS survey is publicly available to researchers. The first time this study was applied was in 1999 to 28 countries and it was called CIVED, the second wave started using the name ICCS and was implemented in 2009 in 38 countries, the last study was performed in 2016 in 24 countries. The next cycle is scheduled for 2022 and 25 countries will participate.  It focuses their research on how young people are prepared to undertake their roles as citizens in a range of countries in the second decade of the 21st century [@iea_international_civic_and_citizenship_education_study_2016_iccs_nodate].  ICCS study evaluates student's knowledge and understanding of civics and citizenship, as well as their attitudes, perceptions, and activities related to civics and citizenship.  

ICCS 2016 addressed the following research questions:  

1. The way civic and citizenship education is implemented in participating countries, including the aim and principles for this learning area, the curricular approaches chosen to provide it, and changes and/or developments since 2009.  
2. The extent of student's knowledge and understanding of civics and citizenship, and the factors associated with its variation across and within countries.  
3. Student's current and expected future involvement in civic-related activities, their perceptions of their capacity to engage in these activities, and their perception of the value of civic engagement.  
4. Student's belief about contemporary civic and civic issues in society, including those concerned with civic institutions, rules, and social principles (democracy, citizenship, and diversity), as well as their perceptions of their communities and threats to the world's future.  
5. The ways in which schools organize civic and citizenship education, with a particular focus on general approaches, the processes used to facilitate civic engagement, interaction with their communities, and schools' and teacher's perceptions of the role of this learning area.  

The 2016 study gathered data from more than 94000 students in 8th grade in about 3800 schools from 24 countries. Also, data from more than 37000 teachers in those schools and contextual data collected from school principals are included. An additional European questionnaire gathered data from almost 53.000 students in 14 European countries and a Latin American student questionnaire from more than 25000 students from 5 Latin American countries.  

Of all 24 participant countries, 16 are from Europe, 5 are from Latin America, and 3 from Asia.  In two of the participant countries, a sub-national entity participates. In Belgium, ICCS 2016 was implemented only in the Flemish education system and North Rhine-Westphalia state in Germany took part as a benchmarking participant. The student population is defined as students in 8th grade, in average 13.5 years of age in this study.  

The school's samples were designed as stratified two-stage cluster samples, first schools were randomly selected at the first stage with probability proportional to the size and intact classrooms were sampled at the second stage. Each country has a sample size of 150 schools approximately and a sample of students around 3000 and 4500. Additionally, around 15 teachers teaching the target grade from each school were sampled.  

The assessment framework identified the different types of student perceptions and behaviors relevant to civics and citizenship along two affective-behavioural domains:   

  i. Attitudes: Refer to judgments or evaluations regarding ideas, persons, objects, events, situations, and/or relationships. This includes the students' belief about democracy and citizenship, students' attitudes towards the rights and responsibilities of groups in society, and students' attitudes towards institutions.   
  ii. Engagement:  Refers to students' civic engagement, students' expectations of future civic related action, and students' disposition to actively engage in society (interest, sense of efficacy). The sense of engagement also includes preparedness to participate in forms of civic protest, anticipated future political participation as adults, and anticipated future participation in citizenship activities.  
  
### Students' endorsement of equal rights and opportunities  

Based on the research questions indicated previously, Students' endorsement of equal rights and opportunities indicators from the attitude's domain are the most suitable scales to be studied. 

ICCS 2016 scale *Students' endorsement of equal rights and opportunities* includes 2 different scales, attitudes towards gender equality and attitudes towards equal rights for all ethnic/racial groups. Items indicated in table \@ref(tab:tableA1) will be used in this study to identify the profiles of students towards equal rights and opportunities.  

Accordingly to the technical report [@iea_international_civic_and_citizenship_education_study_2016_iccs_nodate], a two-dimensional model in a confirmatory latent class analysis (CFA) using the items of these indicators showed a good fit after controlling for the common residual variance between the negatively worded statements on gender equality. These two latent dimensions are highly correlated (0.63) and the measurement invariance was within acceptable ranges, this means that a certain degree of measurement invariance across countries was achieved when considering a variable-centre approach.

The scales *Students' endorsement of gender equality* (S_GENEQL) and *Students' endorsement of equal rights for all ethnic/racial groups* (S_ETHRGHT) created by the consortium using CFA, consist of values ranging from 16.32 to 63.94 and 19.33 to 66.36 respectively for the 14 European countries. The distribution of these indicators can be observed in figure \@ref(fig:scalesDis).  Here, it is possible to identify that most countries average values are similar to the European weighted average on both scales, nonetheless few countries performed below this value, such as Bulgaria, Estonia, Latvia and The Netherlands in both scales. Lithuania performs below the average on the gender equality scale but higher on the ethnic/racial groups equality indicator. On the other hand, Norway and Sweden obtained indicators considerable higher than the European weighted average.

```{r scalesDis, fig.cap="Distribution of derived scales Students' endorsement of equal rights and opportunities"}
mg <- data_model %>% dplyr::select(S_GENEQL, S_ETHRGHT, ws) %>% 
  summarise_at(c("S_GENEQL", "S_ETHRGHT"), list(~ weighted.mean(., ws, na.rm = TRUE))) %>% data.frame() %>% 
  reshape2::melt(value.name = "mean") %>% 
  mutate(variable = case_when(variable == "S_GENEQL" ~ "Students' endorsement \nof gender equality",
                           variable == "S_ETHRGHT" ~ "Students' endorsement of \nequal rights for all ethnic/racial groups"))

data_model %>% select(S_GENEQL, S_ETHRGHT, IDCNTRY, ws) %>% 
  reshape2::melt(id.vars = c("IDCNTRY", "ws"), value.name = "scale") %>% 
  mutate(variable = case_when(variable == "S_GENEQL" ~ "Students' endorsement \nof gender equality",
                           variable == "S_ETHRGHT" ~ "Students' endorsement of \nequal rights for all ethnic/racial groups")) %>% 
  ggplot(aes(x = scale, y = reorder(IDCNTRY, desc(IDCNTRY)), group = IDCNTRY, fill = IDCNTRY)) +
  geom_violin(aes(weight = ws), alpha = 0.5) +
  geom_boxplot(aes(weight = ws), width=0.1) +
  geom_vline(aes(xintercept = mean), mg, linetype="dotted", size = 0.8) +
  facet_grid(. ~ variable, scales = "free_x") +
  theme_bw() +
  #ggtitle("Distribution of derived scales") +
  ylab("European participant countries") +
  xlab("Students' endorsement of equal rights and opportunities") +
  #scale_fill_brewer(type = "qual", palette = "Dark2") +
  xlim(19,70) +
  theme(legend.position = "none",
        plot.title = element_text(size=10),
        axis.title = element_text(size=10),
        strip.text.x = element_text(size = 8))
```

CFA, a variable-centre methodology, was performed to evaluate country invariance in [@isac_indicators_2019], which was achieved using additionally attitudes towards immigration indicators, this type of analysis is helpful to identify global behaviours of the whole population in the country and be able to compare them across countries. However, with this methodology, it is not possible to dig deeper into different subpopulations in each country and explain those average values.    

The rise of engaged citizenship study [@hooghe_rise_2015] used latent class analysis to identify groups supporting duty-based and engaged citizenship norms, based on 12 indicators. The research included 21 countries, although no invariance analysis was evaluated to compare these groups across countries. The analysis was performed both in 1999 and in 2009 scales and the sizes of the groups in both periods are compared globally and for each country.  

Not much research using these indicators has been performed in order to identify how many subpopulations can be identified in each country regarding young people's beliefs about equal rights and opportunities for different groups in society based on gender and ethnic/racial background. Neither how the patterns or behaviours of these subpopulations are composed and which of them are representatives and comparable across countries.   

## Mixture models Latent Class Analysis  


Parameters that describe a factor's effects in an ordinary generalized linear model are called fixed effects. Fixed effect applies to all categories of interest, gender, treatments, or any other manifest grouping variable. By contrast, random effects apply to a sample of all possible categories.  GLM extend ordinary regression by allowing non normal responses and a link function of the mean. The generalized linear mixed model (GLMM) is a further extension that permits random and fixed effects in the linear predictor.    

In this type of analysis, a contingency table is treated as a finite mixture of unobserved tables generated under a conditional independence structure at categories of a latent variable [@agresti_categorical_2013]. 
A GLMM with discrete data create a mixture of linear predictor values using a latent variable, in this case, the unobserved random effect vector instead of being continuous and assumed to have a Normal distribution, it is a qualitative mixture distribution.  

### Person center approach

Meanwhile ANOVA, multiple regression, mixed models are variable-centered approaches that focus on relations among variables and assume that the sample studied come from a homogeneous population. Mixture models (finite mixture models) have taken the place of the framework for a person-center analytic approach. The difference between these two approaches is that person-centered approach focuses on identifying unobserved subpopulations comprised of similar individuals or cases and involves modeling a mixture outcome distribution.

The most common technique to find homogeneous groups based on observed variables is cluster analysis. There are different methods that can be used to identify these groups of cases, but they are lacking in giving statistical indices and test for the optimal number of clusters. The most common techniques to determine the best number of groups are based on tabular o graphical interpretation of the researcher. 

The mixture of different distributions indicates population heterogeneity, this means that observations arise from a finite number of unobserved subpopulations in the target populations. This is key when we want to identify different patterns in the sample. 

Latent class modeling defines a model for the probability of having a particular response pattern. This probability is a weighted average (or mixture) of the class-specific probabilities for these patterns.  The item responses of an individual are mutually independent given the individual's class membership. Like cluster analysis, it is possible to assign individuals to the latent classes. The probability of belonging to a particular class given the responses (posterior class membership probability) can be obtained by the Bayes' rule [@michalos_latent_2014].    

In summary, mixture modeling provides an important complement to traditional variable-centered analytical approaches. It offers the opportunity for researchers to identify unknown a priori homogeneous classes of individuals based on the measures of interest, examine the features of heterogeneity across the classes, evaluate the effects of covariates on the class membership, assess the relationship between the class membership and other outcomes, and study transitions between the latent class memberships over time. As a matter of fact, person-centered approaches and variable-centered approaches can be integrated into a general mixture modeling framework so that one can better understand the relationships among variables and the pattern of such relationships [@muthen_integrating_2000].

### Latent Class Analysis  

As indicated previously a mixture model assumes that some of its parameters differ across unobserved subgroups, latent classes, or mixture components and particularly a latent class model is a mixture model for a set of categorical items. The first LCA approach was improved by [@lazarsfeld_latent_1968] and [@goodman_exploratory_1974].  

A latent class model assumes the existence of a latent categorical variable such that the observed response variables are conditionally independent, given that variable. In other words, LCA can directly assess the theory that distinctive groups of people share specific attitudes. Depending on the response variable in the model, the analysis is called Latent Profile Analysis if is continuous (Normal) and Latent Class Analysis if the response variable is categorical (Multinomial). In this study categorical outcome will be used.   

The goal of LCA is to identify unobserved subgroups based on similar response patterns. In contrast with cluster analysis, LCA is a model-based approach to classify. It identifies subgroups based on posterior membership probabilities rather than dissimilarity measures such as Euclidean or Mahalanobis distance.  The general probability model underlying LCA allows for formal statistical procedures for determining the number of clusters, and more interpretable results stated in terms of probabilities.

LCA assumes conditional independence, that the observed categorical indicators are mutually independent once the categorical latent variable is conditioned out. Assuming the conditional independence, the joint probability of all observed indicator variables is described as: 

\begin{align}
P(u_1,u_1,...,u_Q)= \sum_{k=1}^K{P(C=k)P(u_1|C=k)P(u_2|C=k)...P(u_Q|C=k)} \label{eq01}
\end{align}

From Bayes' formula, the posterior probabilities for each individual to be in different classes are estimated as:  

\begin{align}
P(C=k|u_1,u_2,..u_Q)=\frac{P(C=k)P(u_1|C=k)P(u_2|C=k)...P(u_Q|C=k)}{P(u_1,u_2,...,u_Q)} \label{eq02}
\end{align}

where $P(C=k)$ are the unconditional probabilities ($\sum_{k=1}^KP(C=k)=1$) and $P(u_Q|C=k)$ are the conditional probabilities.  

The unconditional probabilities are latent class probabilities, and the average of the probabi-lities can be interpreted as the prevalence of latent class (relative frequency of class membership) or the proportion of the population expected to belong to a latent class. The conditional probabilities are conditional item-response probabilities, measurement parameters, representing the likelihood of endorsing specific characteristics of the observed items, given a specific class membership.

Conditional probabilities close to 1.0 indicate that members in the corresponding latent class endorse a category of the item; on the contrary, a very small probability indicates that they do not endorse the characteristic of the item. When a conditional item-response probability is close to $1/J$, where J is the number of categories in the item, the conditional probability is considered as random probability, thus the latent class membership is not predictive of the patterns of item response. 

The conditional item-response probability is defined in (\ref{eq03}) and (\ref{eq04}).  

\begin{align}
P(u_q = u_{qj}|C=k) = \frac{1}{1+exp(-L_{jk})} \label{eq03}
\end{align}


\begin{align}
L_{jk}=ln(\frac{P_{jk}}{1-P_{jk}}) \label{eq04}
\end{align}

which is the logit for $u_{qj}$ given in latent class $k$. A logit of 0 means that the conditional item probability $P_{jk}=0.5$, when the logit takes an extreme value as -15 then $P_{jk}=0$. On the contrary, a logit with a positive extreme value 15, $P_{jk}=1$. These conditional item response probabilities provide information about how the latent classes differ from each other, for this reason, are used to define the estimated classes.  

#### Number of classes  

Determining the number of latent classes is the most important part of a Latent Class Analysis. This cannot be estimated directly from the data. To determine the optimal number of classes, a series of LCA models with an increasing number of latent classes should be fitted. The optimal number of classes will be obtained based on the comparison of the k-class model with the (k-1) class model iteratively. 

It is important to consider other aspects before deciding the final number of classes, it is recommended to follow a series of step to identify the model that best fit the underlined classes. 

a) Compare subsequent models by model fit indices.  
b) Evaluate the quality of latent class membership.  
c) Confirm that the size of the latent classes is reasonable.  
d) Identify that the final classes are interpretable based on a theoretical grounding.  

#### Model fit 

In mixture models, multiple model fit statistics can be used to compare models. Information criterion indices, such as AIC, consistent AIC, BIC, aBIC, Lo-Mendell-Rubin likelihood ratio (LMR LR) test, adjusted LMR LR test and bootstrap likelihood ratio test (BLRT) are described in following section.

##### Akaike's Information Criterion  

\  

Akaike's Information Criteria called AIC, is one of the more important indicators to evaluate models performance, where $length(M)$ in (\ref{eq1}) corresponds to the length of parameter vector of the model $M$. AIC penalizes the log-likelihood, generating a balance between a good fit (high value of log-likelihood) and complexity (simple models are preferable).  


\begin{align}
AIC(M) = -2 \: log-likelihood_{max}(M) + 2 \:length(M) \label{eq1}
\end{align}


AIC prefers a model with few parameters, but the fit of the model is good as well. Numerical results have shown that AIC tends to overfit, it tends to pick models with more parameters than strictly necessary. It can be proven that this effect tends to vary in one parameter more than necessary. The corrected version of AIC can be express as in (\ref{eq2}).

\begin{align}
AIC_c{f(\theta)} = AIC{f(.;\theta)} + \frac{2 \: length(\theta)(length(\theta)+1)}{n-length(\theta) - 1} \label{eq2}
\end{align}

##### Bayesian information criterion 

\    

Based on the probability given the data it is possible to find the best model. This idea is based on Bayesian framework, involving prior probabilities on the candidate models along with prior densities on all parameters in the models. In (\ref{eq3}) $n$ is the sample size and $length(\theta)$ the number of parameters. 

\begin{align}
BIC{f(.;\theta)} = -2 \: logL(\hat\theta)+log(n)length(\theta) \label{eq3}
\end{align}

Compared to AIC, BIC include a more severe penalty for complexity. Smaller values of information criterion indices indicate a better model fit. 

##### Log-likelihood ratio test  

\  

The LR test based on model $\chi^2$ statistic is not appropriate in this case, this is because the contingency table usually has many zero cells, for this, the model $\chi^2$ distribution is not correct. In addition, the model with (k-1)-classes is a special case of the k-classes model where the one latent class probability is set to zero, and the difference of the log-likelihood between these two models does not follow a $\chi^2$ distribution.

Lo, Mendell, and Rubin developed the LMR LR test, which is not based on $\chi^2$ distribution but on a correctly derived distribution. A significant P-value ($p<0.05$) of the LMR LR when comparing model fit in a k-classes and (k-1)-class model indicates a significant improvement in model fit in the k-class model compared to the (k-1)-classes model. Then, if the test is statistically insignificant ($P\ge 0.05$) when comparing the (k+1)-class model with the k-class model, this means that there is no more significant improvement in model fit when including a new class, thus cannot reject the k-class model. Consequently, the optimal number of classes will be k.  

LMR LR test may inflate Type I error when the sample size is small, for this adjusted LMR LR was proposed by adjusting the number of degrees of freedom and sample size. These two tests can perform identically.  

An alternative LR test based on non-$\chi^2$ distribution is the BLRT, Bootstrap log-likelihood ratio test where parametric bootstrapping was used to generate a set of bootstrap samples using the parameters estimates from the (k-1)-class model, and each of the bootstrap samples is analyzed for both k-class and (k-1)-class models. A distribution of the log-likelihood differences between the k-class and (k-1)-class model from all the bootstrap samples is constructed. The BLRT is applied following this empirical distribution of the log-likelihood differences. The P-values are interpreted in the same way as the LMR LR test.  

#### Quality of latent class membership classification  

Once the optimal number of classes is identified, the cases or individuals are classified into latent classes. The probability for an individual to be assigned to a specific latent class is measured by posterior class-membership probability given the individual's response pattern on the observed categorical indicators/items. The latent class memberships of individuals are not definitely determined but based on their highest posterior class-membership probabilities.  

If the posterior probability of an individual is close to 1.0, then the class misclassification or uncertainty is small. The probability for correct class-classification for an individual is the highest probability to be in a class, and the probability of misclassification is the sum of the probability to be classified in the rest of the classes. Posterior probabilities for a specific class of 1.0 are unlikely, consequently zero for the rest of the classes. A rule of thumb for acceptable class classification is 0.70 or greater [@nagin_group-based_2005].  

For assessing the quality of class membership classification another criterion is Entropy, whose values range from 0 to 1 with smaller values indicating a better classification as in (\ref{eq4}) where $P_{ik}$ is the posterior probability for the $i$th individual to be in class k. 

\begin{align}
EN(k) = - \sum^N_{i=1} \sum^K_{k=1}P_{ik} ln P_{ik} \label{eq4}
\end{align}

 

##### Relative entropy  

\  

The relative entropy that is defined by [@kamakura_market_2000] as in (\ref{eq5}) for a $k$-class model with a sample size of $N$. This rescaled version of entropy ranges from 0 to 1 and a value closer to 1.0 indicates better classification. A good classification can be defined as some researchers suggest with an entropy of 0.8 or higher, 0.6 is medium and 0.4 is low relative entropy.  

\begin{align}
REN(k) = 1-\frac{EN(k)}{N ln(K)} \label{eq5}
\end{align}

When defining the final latent classes, it is important to check the size of each class, the percentage of individuals in each class represents the prevalence of the corresponding subpopulation in the target population. To have a meaningful class classification, the sizes should not be too small. 
Latent classes must be theoretically meaningful and interpretable. The researcher needs to define and name the classes based on the patterns of item-response probabilities in that class. For this, the classes identified should make sense and if any class is not theoretically interpretable, the model will not be useful regardless of model fit.  

After the number of latent classes is defined, the class classification should be checked and interpreted. Class counts are estimated based on the posterior class membership probabilities for each individual to be partially a member of all the classes. Another type of latent class count is estimated based on the most likely latent class membership, this means that each individual is assigned to the most likely class. If these two types of counts differ substantially indicated that the class membership misclassification is large. With a perfect classification (entropy = 1) the two counts would be identical.   

##### Avoid local maxima  

\  

A well-known problem of any mixture modeling is that model estimation may not converge on the global maximum of the likelihood, but local maxima, providing incorrect parameter estimates [@goodman_exploratory_1974; @muthen_finite_1999]. The solution is to estimate the model with different sets of random values to ensure the best likelihood [@muthen_mplus_2012; @mclachlan_finite_2000].  

The software used automatically generates 10 random sets of starting values in the initial stage for all model parameters, and then maximum likelihood optimization is carried out for 10 iterations using each of the 10 random set starting values, and finally 2 starting values for the final stage optimizations. When more than 2 classes are specified, it requests a larger number of random sets of starting values to avoid local maxima of the likelihood.  

## Measurement invariance

Measurement invariance can be defined as a conditional independence property of the measurement model with respect to a set of sub-populations within the parent population (e.g., gender, countries or time). Measurement invariance is an important prerequisite for using multi-indicator assessment instruments to examine group differences. If the measurement properties of an instrument differ between observed groups (non invariance), it is not possible to compare the differences between the groups [@bialowolski_influence_2016; @kankaras_measurement_2011; @michalos_latent_2014].  

The importance of cross-countries comparisons is at the heart of large-scale international surveys. Instruments that assess subjective attitudes (e.g., attitudes towards migrants) and also psychological traits such as perseverance, aims for the validity and comparability of survey results. Reflective latent constructs measured through self-reports, for example, are particularly affected by subtle linguistic differences in the translated questionnaires and by broader cultural differences. These may introduce variation in participants’ understanding of survey questions, and therefore in the relationship between their responses and the target latent construct. Similarly, when confronted with Likert items (*Strongly Agree*, *Agree*, *Disagree*, *Strongly disagree*), or with subjective rating scales (*on a scale from 1 to 10*), cultural norms may mediate the response process of participants. As a result, international surveys may fall short of their objective to facilitate comparisons across countries [@noauthor_invariance_2019].  

Multigroup Latent Class Analysis tests whether the number of classes is stable across the known groups and if the measurement part of the model is equivalent across these groups.  

#### 1. Heterogenous model

The first model to measure invariance is an *unconstrained model* in which the compared groups exhibit the same number of classes but the parameters defining those classes are freely estimated across groups. This means that assumes that the only similarity between groups is the number of classes identified and allows that response patterns (conditional probabilities) and class sizes vary among groups. Although the number of classes in all groups may be the same, direct between-country comparisons are not possible in this step because the meaning of latent classes may be substantially different. A completely unrestricted multi-group latent class model is equivalent to the estimation of a separate 3-class LC model for each group [@davidov_cross-cultural_2011].  


#### 2. Partial homogeneity  

The second model to test is the *semi-constrained model* in which equality constraints are imposed across the observed groups.  The measurement part of the model (conditional probabilities) is restricted to be equal in all observed groups. For each group, the meaning of latent classes is invariant of the group and cross-group comparisons are meaningful. Yet, the size of the classes (i.e., the relative importance of each class) may still vary. Most applicable and desirable in cross-cultural studies.  

To test for invariance, the unconstrained model and the semi-constrained models are compared using the likelihood ratio test (LRT) and information criteria such as AIC, BIC, aBIC. A statistically significant LRT indicates a substantial decrease in model fit such that the semi-constrained model should be rejected.  The  model with the smallest AIC, BIC, aBIC value is selected as the best-fitting model.  

If the semi-constrained model is rejected, this means, lower information criteria for the unconstrained model and LRT statistically significant, there is no evidence to assume measurement invariance. In this case, latent classes are characterized different across the observed groups and differences in the prevalence of the profiles across the groups cannot be meaningfully determined.  

For invariance to exist, the semi-constrained model should show a better fit to the data than the unconstrained model. Only after establishing the stability of the classes definition across the different groups, it is possible to compare groups and evaluate the differences in class prevalence.

#### 3. Complete homogeneity  

The stricter level of invariance is where all parameters are constrained across countries, and the prevalence of latent classes are restricted to be equal across groups (i.e. the percentage of individuals assigned to different classes will be equal in all groups). This last assumption will imply that the identified groups of individuals with similar scoring patterns are identical in all the groups with identical numbers of individuals assigned to each group.

If the fully constrained model fit best, it can be concluded that there are no differences in how the known groups are represented in each profile. In contrast, if the fully constrained model is rejected but the semi-constrained model holds means that although the profiles have the same meaning in each group, there are differences in how the individuals are distributed across classes.  

Meeting this last assumption ensures the highest level of cross-country comparability but may be difficult to achieve in cross-cultural studies.  

When the number of observations per group is small, likelihood ratio tests have limited power; while with large groups, violations of invariance detected in such tests may be inconsequential for the substantive inferences [@noauthor_invariance_2019]. The problem is compounded by the fact that in realistic settings (when violations of measurement invariance may be due to cultural or language specificities), the hypotheses are not independent, neither across items nor across groups.  

#### Special cases   

In case that the fully constrained and semi-constrained models are rejected, it can be studied if some latent classes are measurement invariant or not and/or if some items are invariant or not. That means that the assumption of measurement invariance can be relaxed for some classes and/or items. This can be done successively until one finds such a less restrictive model that does not fit the data worse than the totally unrestricted model.  

If the number of classes differs between groups, then it can be tested whether the classes that are present in all groups are measurement invariant or not. This means if one group has 2 classes and another group has 3 classes a 3-class multigroup model with full measurement invariance can be tested, where the size of the third class in the first group would be zero. This strategy is recommended for a small number of groups. When a large number of groups is tested another strategy is recommended as it will take so much time in computing and compare all parameters to identify the ones that are invariant or that should be free.  

The appropriate strategy for many groups is to conduct a multigroup LCA where full measurement invariance is assumed across the groups and that the number of classes does not differ across those groups. For this, the appropriate number of classes should be identified for each group and test if just one class is different between them, if this is rejected an extra class should be added to identify if there are two different classes among them. If the double of classes is found as the best fit means that none of the classes is measurement invariant, because different classes by country are needed. This strategy has the advantage to have a higher power to detect small classes that exist in several groups but that would not be detected in country-specific analysis because their size within a group might be too small. 

\clearpage

# Methods  

In this section, how the procedure will be conducted is explained, focusing on the steps used to produce the expected evidence. The methodological features to perform a Latent Class Analysis are important to be clarified firstly to avoid confusion when the process is performed. Exploratory and Confirmatory approaches are explained along with how the invariance will be studied. How the models will be evaluated is described in detail including the factors and criteria to be considered. Due to the amount of information that is necessary to analyze an analytical strategy is fundamental to be able to obtain in few steps the desired results. Data to be used is also identified here, not only the items or variables that will be analyzed but the countries to be included as well.   

## Methodological features  

There are two different approaches to conduct a Latent Class Analysis, an exploratory and confirmatory approach. Both methodologies are valid for this analysis, instead their main difference resides in the hypothesis that wants to be tested.  

When researchers do not have specific hypotheses, but the goal is to identify how many classes are necessary to fit the data, an exploratory latent class analysis can be performed. In this case, several latent class models with an increasing number of classes should be computed. The best-fitting will be selected, this can be identified by which increasing the number of latent classes would not result in a model that fits the data significantly better than the previous model. Information criteria such as AIC, BIC and aBIC can be used to determine the best fit, the best model will have the lowest values of information criteria.  

The confirmatory approach starts with specific hypotheses about the latent structure; the researcher can test if there is a defined number of classes that explains the associations between the observed variables and specific relations in the items for each class or across classes.  Based on the conditional response probabilities and class sizes computed by the software, the expected frequencies can be estimated. These frequencies can be compared with the observed frequencies with a statistical test such as Pearson test or the LRT. If the test statistics show that the observed and expected frequencies do not differ significantly, the model is appropriate to explain the associations of the observed variables.  The expected frequency of each possible response pattern should be at least 1 or even 5 to make sure that both statistics follow a $\chi^2$ distribution and that p-values can be used for a valid decision. In case of sparse tables, bootstrapping goodness of fit is highly recommended [@oecd_invariance_2019].  

As mentioned before, studying measurement invariance is necessary to determine whether the number and nature of the latent profiles are the same across the different observed groups [@olivera-aguilar_assessing_2018]. For this, multigroup LCA models are computed, and the relative fit of the unconstrained and semi-constrained models are compared using the LRT, AIC, BIC, and aBIC measures, Entropy and LL reduction is evaluated as well. Additionally, is needed to review any kind of response bias, the most common refers to "a systematic tendency to respond to a range of questionnaire items on some basis other than the specific item content" for example, extreme responses for agree/disagree [@kankaras_measurement_2011]. 

## Model evaluation  

To estimate and identifying a good LCA model, several factors should be considered after estimating the conditional item response probability [@nylund-gibson_ten_2018]. For the exploratory approach, the selection of the optimal number of classes, examining the latent class classification, labeling the latent classes, and predicting the latent class membership should be performed and evaluated [@wang_structural_2020]. For the confirmatory approach and the measurement invariance study, it is required to evaluate that the model fits properly or better than others according to multiple criteria.  


#### Number of classes  

Determining the number of latent classes is the most important part of a exploratory Latent Class Analysis. This cannot be estimated directly from the data. To determine the optimal number of classes, a series of LCA models with an increasing number of latent classes should be fitted. The optimal number of classes will be obtained based on the comparison of the k-class model with the (k-1) class model iteratively. 

It is important to consider other aspects before deciding the final number of classes, it is recommended to follow a series of steps to identify the model that best fits the underlined classes. 

a) Compare subsequent models by model fit indices.  
b) Evaluate the quality of latent class membership.  
c) Confirm that the size of the latent classes is reasonable.  
d) Identify that the final classes are interpretable based on a theoretical grounding.  

#### Model fit 

In mixture models, multiple model fit statistics can be used to compare models [@michalos_simultaneous_2014]. Information criterion indices, such as AIC, BIC, aBIC, Lo-Mendell-Rubin likelihood ratio (LMR LR) test, adjusted LMR LR test and bootstrap likelihood ratio test (BLRT) are described in the following section according to [@wang_structural_2020].

\newpage

##### Akaike's Information Criterion  

\  

Akaike's Information Criteria called AIC (Akaike, 1973, 1983), is one of the more important indicators to evaluate models performance, where $length(M)$ in (\ref{eq1}) corresponds to the length of parameter vector of the model $M$. AIC penalizes the log-likelihood, generating a balance between a good fit (high value of log-likelihood) and complexity (simple models are preferable).  


\begin{align}
AIC(M) = -2 \: log-likelihood_{max}(M) + 2 \:length(M) \label{eq1}
\end{align}


AIC prefers a model with few parameters, but the fit of the model is good as well. Numerical results have shown that AIC tends to overfit, it tends to pick models with more parameters than strictly necessary. It can be proven that this effect tends to vary in one parameter more than necessary. The corrected version of AIC can be expressed as in (\ref{eq2}).

\begin{align}
AIC_c{f(\theta)} = AIC{f(.;\theta)} + \frac{2 \: length(\theta)(length(\theta)+1)}{n-length(\theta) - 1} \label{eq2}
\end{align}

##### Bayesian information criterion 

\    

Based on the probability given the data it is possible to find the best model. This idea is based on Bayesian framework, involving prior probabilities on the candidate models along with prior densities on all parameters in the models (Schwarz, 1978). In (\ref{eq3}) $n$ is the sample size and $length(\theta)$ the number of parameters. 

\begin{align}
BIC{f(.;\theta)} = -2 \: logL(\hat\theta)+log(n)length(\theta) \label{eq3}
\end{align}

Compared to AIC, BIC includes a more severe penalty for complexity. Smaller values of information criterion indices indicate a better model fit. 

##### Log-likelihood ratio test  

\  

The LR test based on the model $\chi^2$ statistic is not appropriate in this case, this is because the contingency table usually has many zero cells, for this, the model $\chi^2$ distribution is not correct. In addition, the model with (k-1)-classes is a special case of the k-classes model where the one latent class probability is set to zero, and the difference of the log-likelihood between these two models does not follow a $\chi^2$ distribution.

Lo, Mendell, and Rubin developed the LMR LR test (2001), which is not based on $\chi^2$ distribution but on a correctly derived distribution [@wang_structural_2020]. A significant P-value ($p<0.05$) of the LMR LR when comparing model fit in a k-classes and (k-1)-class model indicates a significant improvement in model fit in the k-class model compared to the (k-1)-classes model. Then, if the test is statistically insignificant ($P\ge 0.05$) when comparing the (k+1)-class model with the k-class model, this means that there is no more significant improvement in model fit when including a new class, thus cannot reject the k-class model. Consequently, the optimal number of classes will be k.  

LMR LR test may inflate Type I error when the sample size is small, for this adjusted LMR LR was proposed by adjusting the number of degrees of freedom and sample size. These two tests can perform identically.  

An alternative LR test based on non-$\chi^2$ distribution is the BLRT, Bootstrap log-likelihood ratio test where parametric bootstrapping was used to generate a set of bootstrap samples using the parameters estimates from the (k-1)-class model, and each of the bootstrap samples is analyzed for both k-class and (k-1)-class models. A distribution of the log-likelihood differences between the k-class and (k-1)-class model from all the bootstrap samples is constructed. The BLRT is applied following this empirical distribution of the log-likelihood differences. The P-values are interpreted in the same way as the LMR LR test.  

#### Quality of latent class membership classification  

Once the optimal number of classes is identified, the cases or individuals are classified into latent classes. The probability for an individual to be assigned to a specific latent class is measured by posterior class-membership probability given the individual's response pattern on the observed categorical indicators/items. The latent class memberships of individuals are not definitely determined but based on their highest posterior class-membership probabilities [@wang_structural_2020].  

If the posterior probability of an individual is close to 1.0, then the class misclassification or uncertainty is small. The probability for correct class-classification for an individual is the highest probability to be in a class, and the probability of misclassification is the sum of the probability to be classified in the rest of the classes. Posterior probabilities for a specific class of 1.0 are unlikely, consequently zero for the rest of the classes. A rule of thumb for acceptable class classification is 0.70 or greater [@nagin_group-based_2005].  

For assessing the quality of class membership classification another criterion is Entropy, with values that range from 0 to 1 where smaller values indicate a better classification as in (\ref{eq4}) where $P_{ik}$ is the posterior probability for the $i$th individual to be in class k. 

\begin{align}
EN(k) = - \sum^N_{i=1} \sum^K_{k=1}P_{ik} ln P_{ik} \label{eq4}
\end{align}

 

##### Relative entropy  

\  

The relative entropy that is defined by [@kamakura_market_2000] as in (\ref{eq5}) for a $k$-class model with a sample size of $N$. This rescaled version of entropy ranges from 0 to 1 and a value closer to 1.0 indicates better classification. A good classification can be defined as some researchers suggest with an entropy of 0.8 or higher, 0.6 is medium and 0.4 is low relative entropy.  

\begin{align}
REN(k) = 1-\frac{EN(k)}{N ln(K)} \label{eq5}
\end{align}

When defining the final latent classes, it is important to check the size of each class, the percentage of individuals in each class that represents the prevalence of the corresponding subpopulation in the target population. To have a meaningful class classification, the sizes should not be too small [@wang_structural_2020]. 
Latent classes must be theoretically meaningful and interpretable. The researcher needs to define and name the classes based on the patterns of item-response probabilities in that class. For this, the classes identified should make sense and if any class is not theoretically interpretable, the model will not be useful regardless of model fit.  

After the number of latent classes is defined, the class classification should be checked and interpreted. Class counts are estimated based on the posterior class membership probabilities for each individual to be partially a member of all the classes. Another type of latent class count is estimated based on the most likely latent class membership, this means that each individual is assigned to the most likely class. If these two types of counts differ substantially indicates that the class membership misclassification is large. With a perfect classification (entropy = 1) the two counts would be identical.   

##### Avoid local maxima  

\  

A well-known problem of any mixture modeling is that model estimation may not converge on the global maximum of the likelihood, but local maxima, providing incorrect parameter estimates [@goodman_exploratory_1974; @muthen_finite_1999]. The solution is to estimate the model with different sets of random values to ensure the best likelihood [@muthen_mplus_2012; @mclachlan_finite_2000].  

The software used automatically generates 10 random sets of starting values in the initial stage for all model parameters, and then maximum likelihood optimization is carried out for 10 iterations using each of the 10 random set starting values, and finally 2 starting values for the final stage optimizations. When more than 2 classes are specified, it requests a larger number of random sets of starting values to avoid local maxima of the likelihood.   

## Analytical strategy

To perform the analysis for this research the strategy to be used is based on performing firstly independent analysis for each selected country, this way it will be possible to identify the country-specific number of classes as a starting point.  

Based on the optimal number of classes for each country, an independent analysis of the latent class will be performed to evaluate class membership, sizes and the interpretation of these classes for each country.  

Once all the classes are identified for each country, all these classes will be compared between countries to identify similar and unique classes across countries.  With this, a final number of optimal classes will be chosen for the total sample (including all the countries) to absorb all the possible different classes found in all the countries independently. If similar classes were found in more than one country this will count as one in the global model, if unique classes are identified this will count as a country-specific class. This country-specific class could be different across countries for that reason it will count as just one class. If one country or more has two or more country-specific classes, two or more classes will be included in the global model.  

With the number of classes decided for the global model an exploratory latent class analysis is performed where a number of classes will be similar across countries and the remaining classes will differ across countries. The results from this model will be used to evaluate the model fit and the results will be compared between including/excluding one class.  

The model with a good fit and the best interpretability will be chosen to evaluate the levels of invariance. First, the heterogeneous model will be analyzed, where all the parameters are freely estimated across countries. This will be similar to trying different models for each country. This model is going to be compared to more restricted models. Next, some parameters will be fixed to test if partial invariance can be achieved, with this it will be tested if the class patterns are comparable across countries. As a third step, the most restricted model is computed, where not only the patterns are assumed to be equal across countries but also the class membership. 

Finally, confirmatory latent class analysis will be evaluated with some fixed conditional probabilities based on hypotheses obtained from the global and most invariant model identified. 

The confirmatory approach will be evaluated using the same sample used in the exploratory approach, due to the complex nature of the data it is not possible to split the data into training and test data using the sampling weights. This approach is mainly performed to specify obvious relations that could be observed within and between the identified patterns. 

In summary, this strategy will allow for identifying if there are some classes and how many of them can be compared across countries and how we can interpret them.  

1. Independent exploratory LCA to identify the country-specific number of classes.
2. Independent analysis to identify country-specific latent class membership, sizes and interpretability.
3. Identify similar classes across countries.
4. Exploratory LCA with all groups to check if the total number of classes remains.
5. Evaluate different levels of measurement invariance for the number of comparable classes across countries.
6. Establish a confirmatory analysis with the conditional probabilities and the total number of classes identified in the general exploratory LCA model.

The analytical strategy will be performed using both software, R and Mplus. R has been extensively used when manipulating large-scale assessment data because of its flexibility to work with multiple datasets at the same time and combine them maintaining all their original features [@carrasco_r4sda_2021; @mirazchiyski_r_2021]. Mplus is a specialized software to perform Mixed models, among other techniques, especially with complex samples. This software allows to include weights, clustering, and stratification variables. This is the core for modeling large scale assessment data. Mplus can be used by creating automatized code in R with MplusAutomation package [@hallquist_mplusautomation_2018], that allows it to extract the output of the complex procedure performed and utilize R features to summarize and report the results. Code used for the different models are summarized in appendix A.3.

To avoid local maxima and obtain trustworthy estimations the number of random sets of starting values for initial stage optimization was set to 100, the number of random sets of starting values for final stage optimization to 25 (a quarter of the number of initial starting values) and the maximum number of iterations in optimization to 5.  

To be completely sure that the model has reached the global maximum value of likelihood, Mplus performs the model by running the analysis multiple times and indicating if the global maxima is reached. When this is not the case it is not possible to compare the results, this is clearly stated in the following chapter.

## Data   

In this section, the different scales used in the research are explained. Firstly, an explanation of the variables that conform to every scale used in the analysis are described. Secondly, a summary and description of the data selected for the analysis, characteristics and size are given.  

ICCS 2016 indicator *Students' endorsement of equal rights and opportunities* measure two constructs, attitudes towards gender equality and attitudes towards equal rights for all ethnic/racial groups. Accordingly to the technical report [@wolfram_schulz_iccs_2018], a two-dimensional model in a confirmatory latent class analysis (CFA) using the items of these indicators showed a good fit after controlling for the common residual variance between the negatively worded statements on gender equality. These two latent dimensions are highly correlated (0.63) and the measurement invariance was within acceptable ranges, this means that a certain degree of measurement invariance across countries was achieved when considering a variable-centered approach.

The scales *Students' endorsement of gender equality* (S_GENEQL) and *Students' endorsement of equal rights for all ethnic/racial groups* (S_ETHRGHT) created by the consortium using the individual items with Confirmatory Factor Analysis, consist of values ranging from 16.32 to 63.94 and 19.33 to 66.36 respectively for the 14 European countries. The distribution of these indicators can be observed in figure \@ref(fig:scalesDis).  Here, it is possible to identify that most countries' average values are similar to the European weighted average on both scales (dotted line), nonetheless few countries performed below this value, such as Bulgaria, Estonia, Latvia and The Netherlands in both scales. Lithuania performs below the average on the gender equality scale but higher on the ethnic/racial groups equality indicator. On the other hand, Norway and Sweden obtained indicators considerably higher than the European weighted average.


```{r scalesDis, fig.cap="Distribution of derived scales Students' endorsement of equal rights and opportunities ICCS 2016"}
mg <- data_model %>% dplyr::select(S_GENEQL, S_ETHRGHT, ws) %>% 
  summarise_at(c("S_GENEQL", "S_ETHRGHT"), list(~ weighted.mean(., ws, na.rm = TRUE))) %>% data.frame() %>% 
  reshape2::melt(value.name = "mean") %>% 
  mutate(variable = case_when(variable == "S_GENEQL" ~ "Students' endorsement \nof gender equality",
                           variable == "S_ETHRGHT" ~ "Students' endorsement of \nequal rights for all ethnic/racial groups"))

data_model %>% select(S_GENEQL, S_ETHRGHT, IDCNTRY, ws) %>% 
  reshape2::melt(id.vars = c("IDCNTRY", "ws"), value.name = "scale") %>% 
  mutate(variable = case_when(variable == "S_GENEQL" ~ "Students' endorsement \nof gender equality",
                           variable == "S_ETHRGHT" ~ "Students' endorsement of \nequal rights for all ethnic/racial groups")) %>% 
  ggplot(aes(x = scale, y = reorder(IDCNTRY, desc(IDCNTRY)), group = IDCNTRY, fill = IDCNTRY)) +
  geom_violin(aes(weight = ws), alpha = 0.5) +
  geom_boxplot(aes(weight = ws), width=0.1) +
  geom_vline(aes(xintercept = mean), mg, linetype="dotted", size = 0.6) +
  facet_grid(. ~ variable, scales = "free_x") +
  theme_bw() +
  #ggtitle("Distribution of derived scales") +
  ylab("European participant countries") +
  xlab("") + #Students' endorsement of equal rights and opportunities") +
  #scale_fill_brewer(type = "qual", palette = "Dark2") +
  xlim(19,70) +
  theme(legend.position = "none",
        plot.title = element_text(size=10),
        axis.title = element_text(size=10),
        strip.text.x = element_text(size = 8))
```

As mentioned in the previous chapter, these scales provide valuable information on a country's average level. For the purpose of this thesis, to identify subpopulations within each country, using each item that composes these scales separately will provide more information about the patterns that can be observed. For this, each item that produces both scales are described below.

The first scale *Students' endorsement of gender equality* is composed of seven items present also in ICCS 2009, asking about the roles of women and men in society (table \@ref(tab:tableA1)). Students were asked to indicate their level of agreement (four levels ranging from "strongly agree" to "strongly disagree") with each statement. The first three items consult about the level of agreement with statements related to participation in government, equal right for work and equal pay, the items are worded as *"Men and women should have equal opportunities to take part in government"*, *"Men and women should have the same rights in every way"* and *"Men and women should get equal pay when they are doing the same jobs"*. 

Next, three items which are negatively worded consult for the level of agreement with statements related to women politics participation and two competitive items in favor towards men, *"Women should stay out of politics"*, *"When not many jobs available, men should have more right to a job than women"* and *"Men are better qualified to be political leaders than women"*. These last items were inversely coded, this means that when an individual responded "Agree" to any of these items, this response was coded as "Disagree" and then the question should be interpreted inversely. In the results chapter, these items will appear with a "(r)" added at the end of the label to easily identify them. Higher values of this scale reflect stronger agreement with the notion of gender equality or stronger disagreement with negative views of gender equality. 

The other set of questions for the scale *Students' endorsement of equal rights for all ethnic/racial groups* are focused on the rights and responsibilities of all different ethnic/racial groups in society. Same as before students indicate their level of agreement in the same range (four levels ranging from "strongly agree" to "strongly disagree"). This scale indicates with higher scores a greater degree of agreement with the idea that ethnic and racial groups should have the same rights as other citizens in society.  

In this scale, also from ICCS 2009, the first two items are focused on evaluating the attitude towards equality in education and work with *"All ethnic and racial groups should have equal chance to get a good education"* and *"All ethnic and racial groups should have an equal chance to get good jobs"*.  The third item is related to respect with *"Schools should teach students to respect members of all ethnic and racial groups"* and last two items are focused on political equality and social responsibilities with *"Members of all ethnic and racial groups should be encouraged to run in elections for political office"* and *"Members of all ethnic and racial groups should have same rights and responsibilities"*.  

The original response categories for these items are based on four points agree/disagree scale, starting by the lower level of agreement to the strongest, "Strongly disagree" (1), "Disagree" (2), "Agree" (3), and "Strongly agree" (4). For this analysis and to be able to reduce the complexity in the interpretation, these categories were recoded into two levels, "Disagree" (1-2) and "Agree" (3-4).  

Multiple countries participate in the ICCS 2016 study, from different continents Europe, Asia, Latin America and the Caribbean mainly (detailed sample size of the participating countries can be found in Table \@ref(tab:tableA2) in the appendix). 

Only European countries were selected for this research, the decision was mainly based on choosing countries with different backgrounds, where no characteristics such as language, geographical location, economic status, or others could influence unwanted factors that could impact the results.  
Fourteen countries were chosen, from nordic, western, central, eastern, and southern Europe. Countries' sample sizes differ, the country with the highest student sample is Norway (6271), followed by Denmark (6254), and the countries with the lowest sample size are Netherlands (2812) and Slovenia (2844). Regardless the different sample sizes, senate weights are used in the analysis, the advantage of this type of weights is that balance the number of observations in a way that the total number of observations for each country weights the same overall. This will prevent bigger samples from influencing the global results, that way all countries participate at the same level.    

Some considerations to be aware of the samples according to the ICCS 2016 User Guide for the International Database [@kohler_iccs_2018] are:  
- Malta assessed Grade 9 students, because the average age of Grade 8 students in Malta is below 13.5 years old.  
- Norway (Grade 9) deviated from the internationally defined population and surveyed the adjacent upper grade.  
- Exclusion rates pertaining to the student population were greater than five percent in Estonia, Latvia, Norway (Grade 9), Sweden and North Rhine-Westphalia (Germany). The ICCS 2016 research team deemed this level of exclusion a significant reduction of the target population coverage, and researchers need to keep this caveat in mind when interpreting results.
 
The need of using a complex sample design forces the analysis to be even more complex as if there were no weights involved in the sample. Luckily, Mplus provides a set of tools that allows performing a latent class analysis and multigroup LCA considering not also the complexity of the sample design, but the inclusion of the student senate weights in the estimation as well.  

Generally, MPLUS software can perform most of the analysis needed, still some tests such as the bootstrap likelihood ratio difference test for comparing models differing in the number of classes is not possible to calculate when using sampling weights. This is a disadvantage when working with representative samples. Nonetheless the rest of the analysis could be performed.  

\clearpage

